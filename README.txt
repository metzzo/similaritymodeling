For the first Part of Similarity Modelling we started out by extracting the audio tracks from the videos to have the ability to analyse them seperatly. Our initial impression from looking at the files was, that the Jumps are rather visible when viewing the audio files in spectrogram view in audacity. Our initial idea was to just detect the jumps by finding these peaks in the audio. Because we wanted to initially stay away from Fourier Tranformation Features we used Zero Crossing Rate and Root Mean Square as our features and tried to manually find treshholds that would fit the jumps in our data. 

We came up with a few treshhold that worked for at least a few examples but we weren't satisified with this approach, because it didn't generalize well and it didn't use any machine learning. As a next step we used low and conservative treshholds to gather a subset of possible candidates and then fed the subset into a SVM. But with just using the two attributes we couldn't linearly seperate the feature space so that solution fell apart.

Our next idea was to use FFT to detect high values like we saw in the spectogram. For our initial implementation we converted the audio signal into the frequenzy domain using 1.5 second windows, to match the groundtruth which had a second accuracy. We then summed all the value over the 4mhz (from our observation of the spektrogram). This resulted in pretty usefull values overall, but we had a few anomalies, the funniest of which was a point that was always detected as a jump and after listening to the audio we found out that it was just someone laughing really loud. We kind of felt trolled :D

After that we went ahead and tried different typ of spectral features in conjunction with Neural Networks. A big problem we had for a while was deciding on the types of Windowing we wanted to use and how to trim windows down to get relevant spectral components but also applying to the whole window we needed to represent for purposed of our ground truth. Another problem we had was in the way we picked test samples for our neural network to train. Using the whole audio resulted in weird issues where we had way to many negative samples to actually train a network. We first solved this by selecting the positive samples and random negative samples but this seemed to lead to either overfitting for one or the other group.